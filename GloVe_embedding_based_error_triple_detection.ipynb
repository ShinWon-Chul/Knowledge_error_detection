{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pacakges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances  #k-means using euclidean_distances\n",
    "import gensim\n",
    "import scipy\n",
    "import datetime\n",
    "import pickle\n",
    "import gap\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GloVe embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_name options = freebase, dbpedia, wisekb\n",
    "dataname = 'wisekb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_k : dbpedia = 15\n",
    "#         wiseKB = 27\n",
    "#         freebase = 46\n",
    "opt_k=27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'./glove_{dataname}/person_embedding','rb')\n",
    "vector = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "f = open(f'./glove_{dataname}/person_words','rb')\n",
    "word = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    " \n",
    "glove_dict = {}\n",
    "for i in range(len(vector)):\n",
    "    glove_dict[word[i]] = vector[i]\n",
    "    \n",
    "print(len(glove_dict))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dir = f'./data/{dataname}/'\n",
    "\n",
    "train_pos_loc = label_dir + 'train_positive_20000.txt'\n",
    "train_neg_loc = label_dir + 'train_negative_5000.txt'\n",
    "test_pos_loc = label_dir + 'test_positive_5000.txt'\n",
    "test_neg_loc = label_dir + 'test_negative_5000.txt'\n",
    "\n",
    "train_pos_embedding = []\n",
    "train_neg_embedding = []\n",
    "test_pos_embedding = []\n",
    "test_neg_embedding = []\n",
    "\n",
    "if dataname=='freebase':\n",
    "    sep = '\\t'\n",
    "else:\n",
    "    sep = ' '\n",
    "    \n",
    "with open(train_pos_loc) as f:          \n",
    "    for i in f:\n",
    "        train_pos_embedding.append(i.split(sep)[0].strip())\n",
    "        \n",
    "print('train_pos : ',len(train_pos_embedding))\n",
    "print(train_pos_embedding[:5])\n",
    "\n",
    "with open(train_neg_loc) as f:          \n",
    "    for i in f:\n",
    "        train_neg_embedding.append(i.split(sep)[0].strip())\n",
    "        \n",
    "print('\\ntrain_neg : ',len(train_neg_embedding))\n",
    "print(train_neg_embedding[:5])\n",
    "\n",
    "with open(test_pos_loc) as f:       \n",
    "    for i in f:\n",
    "        test_pos_embedding.append(i.split(sep)[0].strip())\n",
    "        \n",
    "print('\\ntest_pos : ',len(test_pos_embedding))\n",
    "print(test_pos_embedding[:5])\n",
    "\n",
    "with open(test_neg_loc) as f:\n",
    "    for i in f:\n",
    "        test_neg_embedding.append(i.split(sep)[0].strip())\n",
    "        \n",
    "print('\\ntest_neg : ',len(test_neg_embedding))\n",
    "print(test_neg_embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run Gap algorithm \n",
    "- to get optimal K for K-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this function first generates B reference samples; for each sample, the sample size is the same as the original datasets;\n",
    "the value for each reference sample follows a uniform distribution for the range of each feature of the original datasets;\n",
    "using a simplify formula to compute the D of each cluster, and then the Wk; K should be a increment list, 1-10 is fair enough;\n",
    "the B value is about the number of replicated samples to run gap-statistics, it is recommended as 10, and it should not be changed/decreased that to a smaller value;\n",
    "\n",
    "parameters:\n",
    "    X: np.array, the original data;\n",
    "    refs: np.array or None, it is the replicated data that you want to compare with if there exists one; if no existing replicated/proper data, just use None, and the function\n",
    "    will automatically generates them; \n",
    "    B: int, the number of replicated samples to run gap-statistics; it is recommended as 10, and it should not be changed/decreased that to a smaller value;\n",
    "    K: list type, the range of K values to test on;\n",
    "    N_init: int, states the number of initial starting points for each K-mean running under sklearn, in order to get stable clustering result each time; \n",
    "    you may not need such many starting points, so it can be reduced to a smaller number to fasten the computation;\n",
    "    n_jobs below is not an argument for this function,but it clarifies the parallel computing, could fasten the computation, this can be only changed inside the script, not as an argument of the function;\n",
    "'''\n",
    "# for debug\n",
    "#X = init_board_gauss(200,4, clear = False)\n",
    "#plt.scatter(X[:,0],X[:,1])\n",
    "\n",
    "# X = np.array([glove_dict[n.split()[0].strip()] for n in train_pos_embedding])\n",
    "# gaps, sk, K = gap.gap_statistic(X)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For K-means algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function to run K-means algorithm\n",
    "\n",
    "parameters:\n",
    "    vector_list : list of vectors to which the K-means algorithm applies\n",
    "    k : value of K to run K-means\n",
    "    \n",
    "return:\n",
    "    X : glove_vectors of input \n",
    "    k_centroid_points : centroid points of k clusters\n",
    "    k_labels : label for the cluster to which each vector belongs to\n",
    "'''\n",
    "def Kmeans_alg(vector_list, k):\n",
    "    X = np.array([glove_dict[n.split()[0]] for n in vector_list])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0,max_iter=1000).fit(X)\n",
    "    k_centroid_points = kmeans.cluster_centers_  # each c_count of center point \n",
    "    k_labels = kmeans.labels_                    # [...] index is vector's index, value is in cluster\n",
    "    \n",
    "    return X, k_centroid_points, k_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function to check which cluster given vector belongs to \n",
    "\n",
    "parameters:\n",
    "    vec_tuple_list : 20000 length list of tuple tuple (entitiy, vector)\n",
    "    delats_ : 1 use initial radius\n",
    "    max_distance_each_cluster : radius of each cluster dict key : cluster, value : radius distance\n",
    "    centroids : centorid point each cluster nparray shape(28,100)    \n",
    "    \n",
    "return:\n",
    "    in_cluster_dict : dict of which cluster a given vector belongs to \n",
    "'''\n",
    "def Grant_to_cluster(vec_tuple_list, delta, max_distance_each_cluster, centroids, removed_clusters=[]):\n",
    "    in_cluster_dict = {}\n",
    "    distance_matrix = euclidean_distances([i[1] for i in vec_tuple_list], centroids) # numpy(20000, number of cluster)\n",
    "\n",
    "    if len(removed_clusters) == len(centroids):\n",
    "        print('There is no cluster left! Experiment Finished!')\n",
    "        return None\n",
    "    \n",
    "    for i in range(len(list(vec_tuple_list))):\n",
    "        distance_with_centroid = distance_matrix[i]\n",
    "        close_cluster = np.argmin(distance_with_centroid) # 해당 entity와 가장 가까운 cluster\n",
    "        \n",
    "        check_cluster = True\n",
    "\n",
    "        while check_cluster == True:\n",
    "            # if close cluster is removed\n",
    "            if close_cluster in removed_clusters:\n",
    "                distance_with_centroid[close_cluster] = max(distance_with_centroid)\n",
    "                close_cluster = np.argmin(distance_with_centroid)\n",
    "            else:\n",
    "                check_cluster = False\n",
    "                \n",
    "        delta = np.float32(delta)\n",
    "        radius_delta = np.round(max_distance_each_cluster[close_cluster]*delta, 5)\n",
    "        min_distance = np.round(min(distance_with_centroid), 5)\n",
    "        \n",
    "        if  radius_delta >= min_distance:\n",
    "            in_cluster_dict[vec_tuple_list[i][0]] = close_cluster\n",
    "        else:\n",
    "            in_cluster_dict[vec_tuple_list[i][0]] = 'x'\n",
    "    \n",
    "    return in_cluster_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countTPFP(true_cluster, false_cluster, centroids):\n",
    "    true_count = {}\n",
    "    for i in range(len(centroids)):\n",
    "        true_count[i] = list(true_cluster.values()).count(i)\n",
    "\n",
    "    true_count['x'] = list(true_cluster.values()).count('x')\n",
    "\n",
    "    false_count = {}\n",
    "    for i in range(len(centroids)):\n",
    "        false_count[i] = list(false_cluster.values()).count(i)\n",
    "\n",
    "    false_count['x'] = list(false_cluster.values()).count('x')\n",
    "\n",
    "    return true_count, false_count\n",
    "\n",
    "def getPR(true_count, false_count, cluster_labels, initial_TP):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    tp_list = []\n",
    "    fn_list = []\n",
    "    fp_list = []\n",
    "    tn_list = []\n",
    "    \n",
    "    for cluster in range(cluster_labels):\n",
    "        TP = true_count[cluster]\n",
    "        FN = initial_TP[cluster] - TP\n",
    "        FP = false_count[cluster]\n",
    "        TN = false_count['x'] \n",
    "\n",
    "        if (TP+FP) == 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = float(TP)/(TP+FP)\n",
    "\n",
    "        if (TP+FN) == 0:\n",
    "            recall = 0.0\n",
    "        else:\n",
    "            recall =float(TP)/(TP+FN)\n",
    "\n",
    "        if (precision+recall) == 0:\n",
    "            f1_score = 0.0\n",
    "        else:\n",
    "            f1_score = float(2*precision*recall) / (precision+recall)\n",
    "            \n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1_score)\n",
    "\n",
    "        tp_list.append(TP)\n",
    "        fn_list.append(FN)\n",
    "        fp_list.append(FP)\n",
    "        tn_list.append(TN)\n",
    "\n",
    "    return precision_list, recall_list, f1_list,\\\n",
    "                tp_list, fn_list, fp_list, tn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPRlist(pos_vector, neg_vector, max_distance_each_cluster, centroids, removed_cluster):\n",
    "    precision_matrix = []\n",
    "    recall_matrix = []\n",
    "    f1_matrix = []\n",
    "    tp_matrix = []\n",
    "    fn_matrix = []\n",
    "    fp_matrix = []\n",
    "    tn_matrix = []\n",
    "    \n",
    "    true_cluster = Grant_to_cluster(pos_vector, 1, max_distance_each_cluster, centroids, removed_cluster)\n",
    "    false_cluster = Grant_to_cluster(neg_vector, 1, max_distance_each_cluster, centroids, removed_cluster)\n",
    "    true_count, false_count = countTPFP(true_cluster, false_cluster, centroids)\n",
    "    if 'x' in true_count.keys():\n",
    "        del(true_count['x'])\n",
    "    initial_TP = [v for k, v in sorted(true_count.items())]\n",
    "\n",
    "    deltas = np.arange(0.6,1.0,0.01)\n",
    "    \n",
    "    for delta in deltas:\n",
    "        true_cluster = Grant_to_cluster(pos_vector, delta, max_distance_each_cluster, centroids, removed_cluster)\n",
    "        false_cluster = Grant_to_cluster(neg_vector, delta, max_distance_each_cluster, centroids, removed_cluster)\n",
    "        true_count, false_count = countTPFP(true_cluster, false_cluster, centroids)\n",
    "\n",
    "        p, r, f1, tp, fn, fp, tn= getPR(true_count, false_count, len(centroids), initial_TP)\n",
    "\n",
    "        precision_matrix.append(p) # precision of each cluster and delta / shape(40, len(centroids)) \n",
    "        recall_matrix.append(r) # recall of each cluster and delta / shape(40, len(centroids))\n",
    "        f1_matrix.append(f1)\n",
    "        tp_matrix.append(tp)\n",
    "        fn_matrix.append(fn)\n",
    "        fp_matrix.append(fp)\n",
    "        tn_matrix.append(tn)\n",
    "\n",
    "    return np.array(precision_matrix), np.array(recall_matrix), np.array(f1_matrix),\\\n",
    "            np.array(tp_matrix), np.array(fn_matrix),\\\n",
    "            np.array(fp_matrix), np.array(tn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOptimalPRlist(pos_vector, neg_vector, max_distance_each_cluster, centroids, removed_cluster, optimaldeltas):\n",
    "    optimalP = []\n",
    "    optimalR = []\n",
    "    optimalTP = []\n",
    "    optimalFP = []\n",
    "\n",
    "    true_cluster = Grant_to_cluster(pos_vector, 1, max_distance_each_cluster, centroids, removed_cluster)\n",
    "    false_cluster = Grant_to_cluster(neg_vector, 1, max_distance_each_cluster, centroids, removed_cluster)\n",
    "    true_count, false_count = countTPFP(true_cluster, false_cluster, centroids)\n",
    "    if 'x' in true_count.keys():\n",
    "        del(true_count['x'])\n",
    "    initial_TP = [v for k, v in sorted(true_count.items())]\n",
    "    \n",
    "    for cluster, delta in enumerate(optimaldeltas):\n",
    "        True_cluster = Grant_to_cluster(pos_vector,delta,max_distance_each_cluster,centroids, removed_cluster)\n",
    "        False_cluster = Grant_to_cluster(neg_vector,delta,max_distance_each_cluster,centroids, removed_cluster)\n",
    "        Truecount, Falsecount = countTPFP(True_cluster, False_cluster, centroids)\n",
    "        \n",
    "        #TP\n",
    "        TP = []\n",
    "        if 'x' in Truecount.keys():\n",
    "            del(Truecount['x'])\n",
    "            \n",
    "        for k,v in sorted(Truecount.items()):\n",
    "            if int(k) == cluster:\n",
    "                optimalTP.append(v)\n",
    "            TP.append(v)\n",
    "            \n",
    "        #FP\n",
    "        FP = []\n",
    "        if 'x' in Falsecount.keys():\n",
    "            del(Falsecount['x'])\n",
    "            \n",
    "        for k,v in sorted(Falsecount.items()):\n",
    "            if int(k) == cluster:\n",
    "                optimalFP.append(v)        \n",
    "            FP.append(v)\n",
    "            \n",
    "        #precision  \n",
    "        precisionList = []\n",
    "        for i in range(len(centroids)):\n",
    "            if (TP[i]+FP[i]) == 0:\n",
    "                precision = float(0.0)\n",
    "            else:\n",
    "                precision = TP[i] / (TP[i]+FP[i])\n",
    "            precisionList.append(precision)\n",
    "        \n",
    "        #recall\n",
    "        recallList = []\n",
    "        for i in range(len(centroids)):\n",
    "            FN = initial_TP[i]-TP[i]\n",
    "            if (TP[i]+ FN)== 0:\n",
    "                recall = float(0.0)\n",
    "            else:\n",
    "                recall = TP[i] / (TP[i]+FN)\n",
    "            recallList.append(recall)\n",
    "\n",
    "        optimalP.append(precisionList[cluster])\n",
    "        optimalR.append(recallList[cluster])\n",
    "        \n",
    "    return optimalP, optimalR, optimalTP, optimalFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## run Kmeans algorithm ##############\n",
    "vec_list, centroids, c_label = Kmeans_alg(train_pos_embedding, opt_k)\n",
    "\n",
    "# dictionary for labeling entity with cluster \n",
    "dicts_each_cluster = {}\n",
    "\n",
    "for c in range(opt_k):\n",
    "    same_Cluster = []    \n",
    "    for idx, label in enumerate(c_label):\n",
    "        if label == c:\n",
    "            same_Cluster.append(vec_list[idx].tolist())    \n",
    "    dicts_each_cluster[c]= list(same_Cluster)\n",
    "\n",
    "# dictionary for each cluster's max Euclidean distance \n",
    "max_distance_each_cluster = {}\n",
    "each_cluster_distance = []\n",
    "\n",
    "for i in dicts_each_cluster.keys():\n",
    "    in_vectors = dicts_each_cluster[i] # list of vectors \n",
    "    tmp_distance = euclidean_distances(in_vectors, centroids[i].reshape(1,-1))\n",
    "    each_cluster_distance.append(tmp_distance)\n",
    "    max_distance_each_cluster[i] = max(tmp_distance)[0].astype(np.float32)\n",
    "    \n",
    "# Get vectors from embedding model \n",
    "train_pos_vector = list(map(lambda x: (x,glove_dict[x]), train_pos_embedding))\n",
    "train_neg_vector = list(map(lambda x: (x,glove_dict[x]), train_neg_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restriction with conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_threshold = len(train_pos_embedding)*0.01\n",
    "precision_threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Check Statistics before delta optimzing \n",
    "true_cluster = Grant_to_cluster(train_pos_vector, 1, max_distance_each_cluster, centroids)\n",
    "false_cluster = Grant_to_cluster(train_neg_vector, 1, max_distance_each_cluster, centroids)\n",
    "true_count, false_count = countTPFP(true_cluster, false_cluster, centroids)\n",
    "\n",
    "result_list = []\n",
    "for cluster in range(len(centroids)):\n",
    "    TP = true_count[cluster]\n",
    "    FP = false_count[cluster]\n",
    "    if (TP+FP) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = TP / (TP+FP)\n",
    "    result_list.append([TP,FP,precision])\n",
    "\n",
    "first_result_df = pd.DataFrame(result_list, columns=['1st_pos', '1st_neg', '1st_precision'])\n",
    "# first_result_df\n",
    "\n",
    "# Restriction on cluster\n",
    "remove_condition_list = []\n",
    "\n",
    "remove_condition_list.append(first_result_df['1st_pos'] < remove_threshold)\n",
    "remove_condition_list.append(first_result_df['1st_precision'] < precision_threshold)\n",
    "\n",
    "remove_cluster = set()\n",
    "for condition in remove_condition_list:\n",
    "    remove_cluster = remove_cluster.union(set(list(condition[condition].index.values)))\n",
    "\n",
    "remove_cluster = sorted(remove_cluster)\n",
    "print(remove_cluster)\n",
    "\n",
    "# 2nd Check\n",
    "true_cluster = Grant_to_cluster(train_pos_vector, 1, max_distance_each_cluster, centroids, remove_cluster)\n",
    "false_cluster = Grant_to_cluster(train_neg_vector, 1, max_distance_each_cluster, centroids, remove_cluster)\n",
    "true_count, false_count = countTPFP(true_cluster, false_cluster, centroids)\n",
    "\n",
    "result_list = []\n",
    "for cluster in range(len(centroids)):\n",
    "    TP = true_count[cluster]\n",
    "    FP = false_count[cluster]\n",
    "    if (TP+FP) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = TP / (TP+FP)\n",
    "    result_list.append([TP,FP,precision])\n",
    "    \n",
    "second_result_df = pd.DataFrame(result_list, columns=['2nd_pos', '2nd_neg', '2nd_precision'])\n",
    "pre_result_df = pd.concat([first_result_df, second_result_df], axis = 1)\n",
    "pre_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Train Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize delta for each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1, TP, FN, FP, TN = getPRlist(train_pos_vector, train_neg_vector, \n",
    "                                              max_distance_each_cluster, \n",
    "                                              centroids, remove_cluster)\n",
    "\n",
    "TP_list = TP[np.array(F1).argmax(0),list(range(TP.shape[1]))]\n",
    "optimal_deltas = np.array(F1).argmax(0)\n",
    "optimal_deltas = (optimal_deltas * 0.01)+0.6\n",
    "optimal_deltas = np.array([d if tp != 0 else 0 for d, tp in zip(optimal_deltas, TP_list)])\n",
    "\n",
    "print('optimaldeltas each clusters : \\n', optimal_deltas)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimalP, optimalR, optimalTP, optimalFP = getOptimalPRlist(train_pos_vector, train_neg_vector, \n",
    "                                                            max_distance_each_cluster,\n",
    "                                                            centroids, remove_cluster,\n",
    "                                                            optimal_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l1 = []\n",
    "for n1, n2, n3, n4, n5 in zip(optimalP, optimalR,  optimalTP, optimalFP, optimal_deltas):\n",
    "    l1.append([n1, n2, n3, n4, n5])\n",
    "    \n",
    "opt_delta_df = pd.DataFrame(l1, columns=['precision', 'recall', 'TP','FP', 'delta'])\n",
    "opt_delta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_parse_df = opt_delta_df[['delta','TP','FP', 'precision']]\n",
    "train_result_df = pd.concat([pre_result_df, tmp_parse_df], axis = 1)\n",
    "train_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test vectors from embedding model \n",
    "test_pos_vector = list(map(lambda x: (x,glove_dict[x]), test_pos_embedding))\n",
    "test_neg_vector = list(map(lambda x: (x,glove_dict[x]), test_neg_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimalP, optimalR, optimalTP, optimalFP = getOptimalPRlist(test_pos_vector, test_neg_vector, \n",
    "                                                            max_distance_each_cluster,\n",
    "                                                            centroids, remove_cluster, optimal_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l1 = []\n",
    "for n1, n2, n3, n4,n5 in zip(optimalP, optimalR, optimalTP, optimalFP, optimal_deltas):\n",
    "    l1.append([n1, n2, n3, n4, n5])\n",
    "    \n",
    "output_df = pd.DataFrame(l1, columns=['precision', 'recall', 'TP','FP', 'delta'])\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test output Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_output_df = output_df[['TP','FP', 'precision']]\n",
    "tmp_output_df.columns = ['Test_TP','Test_FP', 'Test_precision']\n",
    "\n",
    "train_test_df = pd.concat([train_result_df, tmp_output_df], axis = 1)\n",
    "train_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_TP = sum(train_test_df['Test_TP'].values)\n",
    "new_FP = sum(train_test_df['Test_FP'].values)\n",
    "\n",
    "new_precision = new_TP / (new_TP + new_FP)\n",
    "new_recall = new_TP / len(test_pos_embedding)\n",
    "new_f1 = (2*new_precision*new_recall) / (new_precision+new_recall)\n",
    "\n",
    "print('TP : ', new_TP)\n",
    "print('FP : ', new_FP)\n",
    "print()\n",
    "print('precision : ', new_precision)\n",
    "print('recall : ', new_recall)\n",
    "print('F1 : ', new_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With one delta - previous method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Precision_Recall(True_dic,False_dic):\n",
    "    FN = list(True_dic.values()).count('x')\n",
    "    TP = len(True_dic.values()) - FN\n",
    "    TN = list(False_dic.values()).count('x')\n",
    "    FP = len(False_dic.values()) - TN\n",
    "    \n",
    "    if (TP+FP) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = float(TP)/(TP+FP)\n",
    "        \n",
    "    if (TP+FN) == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall =float(TP)/(TP+FN)\n",
    "    \n",
    "    return precision,recall,TP,FP,TN,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deltas = np.arange(0.6,1.0,0.01)\n",
    "\n",
    "result_list = []\n",
    "\n",
    "for i in deltas:\n",
    "    true_cluster = Grant_to_cluster(train_pos_vector, i, max_distance_each_cluster, centroids, remove_cluster)\n",
    "    false_cluster = Grant_to_cluster(train_neg_vector, i, max_distance_each_cluster, centroids, remove_cluster)\n",
    "\n",
    "    precision, recall, TP, FP, TN, FN = Get_Precision_Recall(true_cluster,false_cluster)\n",
    "    if (precision + recall) == 0:\n",
    "        f1_measures = 0\n",
    "    else:\n",
    "        f1_measures = float(2*precision*recall) / (precision+recall)\n",
    "\n",
    "    print(\"deltas : \"+ str(round(i,2)))\n",
    "\n",
    "    result_list.append([i, TP, FN, TN, FP, precision, recall, f1_measures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_delta_df = pd.DataFrame(result_list, columns=['delta', 'TP', 'FN', 'TN', 'FP', 'precision', 'recall', 'F1'])\n",
    "one_delta_df.sort_values(by=['F1'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_delta = one_delta_df.sort_values(by=['F1'], ascending=False).iloc[0]['delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cluster = Grant_to_cluster(test_pos_vector, one_delta, max_distance_each_cluster, centroids, remove_cluster)\n",
    "false_cluster = Grant_to_cluster(test_neg_vector, one_delta, max_distance_each_cluster, centroids, remove_cluster)\n",
    "\n",
    "precision, recall, TP, FP, TN, FN = Get_Precision_Recall(true_cluster,false_cluster)\n",
    "\n",
    "if (precision + recall) == 0:\n",
    "    f1_measures = 0\n",
    "else:\n",
    "    f1_measures = float(2*precision*recall) / (precision+recall)\n",
    "\n",
    "result_list = []\n",
    "\n",
    "result_list.append([one_delta, TP, FN, TN, FP, precision, recall, f1_measures])\n",
    "one_result_df = pd.DataFrame(result_list, columns=['delta', 'TP', 'FN', 'TN', 'FP',  'precision', 'recall', 'F1'])\n",
    "one_result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
