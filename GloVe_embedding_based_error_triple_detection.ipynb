{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Error Detection using K-means and Adaptive Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering positive entity vectors using k-means\n",
    "- Attempt to k-means clustering with an entity vector of triples consisting of <entity, type, person>.\n",
    "- A positive entity means that the entity is a person in the triple of <entity, type, person>.\n",
    "    - (e.g. entity : 'Biden', 'Trump', ...)\n",
    "- Find the maximum Euclidean distance between the centroid of each clusters and the elements included in the cluster.\n",
    "- The maximum Euclidean distance obtained from each cluster means the radius $r$ of the cluster, and entities located between the radius from the centroid are classified as entities of the cluster.\n",
    "\n",
    "## 2. Apply the adaptive clustering method.\n",
    "- Add the negative entity vector to the vector space clustered by the positive entity vector.\n",
    "- A negative entity means that the entity is not a person in the triple of <entity, type, person>.\n",
    "    - (e.g. entity : 'Titanic'(Film), 'New York'(City), ...)\n",
    "- Negative vectors are included in the cluster by calculating the distance between each centroid of the cluster.\n",
    "- Find the optimal $\\delta $(0.6 ~ 1.0) that will maximize the $f1-score$ of each cluster.\n",
    "- The $\\delta $ is multiplied by the cluster radius $r$ to create a new cluster range $r'$.\n",
    "    - $f1-score = \\frac{2\\times precision\\times recall}{precision+recall}$\n",
    "    - $precision = \\frac{TP}{TP+FP}$\n",
    "    - $recall = \\frac{TP}{TP+FN}$\n",
    "    - $TP(True Positive)$ = Number of positive entities included in the cluster  \n",
    "    - $FP(False Positive)$ = Number of negative entities included in the cluster\n",
    "    - $FN(False Negative)$ = The number of positive entities included in the cluster before the cluster range was adjusted but not included after the adjustment.  \n",
    "\n",
    "## 3. Perform error triple detection.\n",
    "- Add the positive and negative vectors of the test data to the vector space.\n",
    "- Apply a new radius $r'$ to each cluster multiplied by the optimal $\\delta $.\n",
    "- Entities included in the cluster are classified as positive entities, and entities not included in any cluster are classified as negative entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions\n",
    "from AdaClustering.ClusteringAlgorithm import kmeans_alg, grant_to_cluster\n",
    "from evaluation.metrics import get_initial_TP, get_ada_matrics, get_optimal_matrics\n",
    "from util.counter import count_element, count_TPFP\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob ,os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances  #k-means using euclidean_distances\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe embedding vectors from file\n",
    "- dataname options :\n",
    "    - dbpedia / freebase / wisekb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'dbpedia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50543\n"
     ]
    }
   ],
   "source": [
    "k_dict = {'freebase' : 11, 'dbpedia' : 20, 'wisekb' : 34}\n",
    "opt_k = k_dict[dataname]\n",
    "\n",
    "f = open(f'./data/GloVeEntityVectors/glove_{dataname}/embedding_vectors','rb')\n",
    "vector = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "f = open(f'./data/GloVeEntityVectors/glove_{dataname}/vector_labels','rb')\n",
    "word = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    " \n",
    "glove_dict = {}\n",
    "for i in range(len(vector)):\n",
    "    glove_dict[word[i]] = vector[i]\n",
    "    \n",
    "print(len(glove_dict)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load entity label from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_positive contains 20000 entities\n",
      "['e_2398311', 'e_4856915', 'e_375534', 'e_401503', 'e_1890861'] \n",
      "\n",
      "train_negative contains 5000 entities\n",
      "['e_2022579', 'e_4008488', 'e_1863291', 'e_96735', 'e_2223783'] \n",
      "\n",
      "test_positive contains 5000 entities\n",
      "['e_1089098', 'e_4119140', 'e_2066827', 'e_1183621', 'e_1708557'] \n",
      "\n",
      "test_negative contains 5000 entities\n",
      "['e_1629807', 'e_2163227', 'e_5848230', 'e_2079695', 'e_1725594'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dataname=='freebase':\n",
    "    sep = '\\t'\n",
    "else:\n",
    "    sep = ' '\n",
    "path = f'./data/{dataname}/'\n",
    "all_files = glob.glob(os.path.join(path, \"*.txt\"))    \n",
    "filename_list = ['train_positive', 'train_negative', 'test_positive', 'test_negative']\n",
    "train_pos_entities = []\n",
    "train_neg_entities = []\n",
    "test_pos_entities = []\n",
    "test_neg_entities = []\n",
    "all_entities = []\n",
    "for file_name in filename_list:\n",
    "    for directory in all_files:\n",
    "        if file_name in directory:\n",
    "            entities = []\n",
    "            data_name = file_name.split('/')[0]\n",
    "            with open(directory) as f:          \n",
    "                for triple in f:\n",
    "                    entities.append(triple.split(sep)[0].strip())\n",
    "            print(f'{data_name} contains {len(entities)} entities')\n",
    "            print(entities[:5], '\\n')\n",
    "            all_entities.append(entities)\n",
    "train_pos_entities, train_neg_entities, test_pos_entities, test_neg_entities = all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dir = f'./data/{dataname}/'\n",
    "\n",
    "# train_pos_loc = label_dir + 'train_positive_20000.txt'\n",
    "# train_neg_loc = label_dir + 'train_negative_5000.txt'\n",
    "# test_pos_loc = label_dir + 'test_positive_5000.txt'\n",
    "# test_neg_loc = label_dir + 'test_negative_5000.txt'\n",
    "\n",
    "# train_pos_entities = []\n",
    "# train_neg_entities = []\n",
    "# test_pos_entities = []\n",
    "# test_neg_entites = []\n",
    "\n",
    "# if dataname=='freebase':\n",
    "#     sep = '\\t'\n",
    "# else:\n",
    "#     sep = ' '\n",
    "    \n",
    "# with open(train_pos_loc) as f:          \n",
    "#     for i in f:\n",
    "#         train_pos_entities.append(i.split(sep)[0].strip())\n",
    "        \n",
    "# print('train_pos : ',len(train_pos_entities))\n",
    "# print(train_pos_entities[:5])\n",
    "\n",
    "# with open(train_neg_loc) as f:          \n",
    "#     for i in f:\n",
    "#         train_neg_entities.append(i.split(sep)[0].strip())\n",
    "        \n",
    "# print('\\ntrain_neg : ',len(train_neg_entities))\n",
    "# print(train_neg_entities[:5])\n",
    "\n",
    "# with open(test_pos_loc) as f:       \n",
    "#     for i in f:\n",
    "#         test_pos_entities.append(i.split(sep)[0].strip())\n",
    "        \n",
    "# print('\\ntest_pos : ',len(test_pos_entities))\n",
    "# print(test_pos_entities[:5])\n",
    "\n",
    "# with open(test_neg_loc) as f:\n",
    "#     for i in f:\n",
    "#         test_neg_entites.append(i.split(sep)[0].strip())\n",
    "        \n",
    "# print('\\ntest_neg : ',len(test_neg_entites))\n",
    "# print(test_neg_entites[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering positive entity vectors using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, centroids, c_label = kmeans_alg(train_pos_entities, opt_k, glove_dict)\n",
    "\n",
    "# dictionary for labeling entity with cluster - key : cluster label / - value : list of vectors\n",
    "dicts_each_cluster = {}\n",
    "for c in range(opt_k):\n",
    "    same_Cluster = []    \n",
    "    for idx, label in enumerate(c_label):\n",
    "        if label == c:\n",
    "            same_Cluster.append(vectors[idx].tolist())    \n",
    "    dicts_each_cluster[c]= list(same_Cluster)\n",
    "\n",
    "# dictionary for each cluster's max Euclidean distance \n",
    "max_distance_each_cluster = {}\n",
    "\n",
    "for i in dicts_each_cluster.keys():\n",
    "    in_vectors = dicts_each_cluster[i] \n",
    "    tmp_distance = euclidean_distances(in_vectors, centroids[i].reshape(1,-1))\n",
    "    \n",
    "    # max_distance_each_cluster - key : cluster lable / - value : max_distance\n",
    "    max_distance_each_cluster[i] = max(tmp_distance)[0].astype(np.float32) \n",
    "    \n",
    "# Get vectors \n",
    "train_pos_vector = list(map(lambda x: (x,glove_dict[x]), train_pos_entities)) # [(word, vector), ... (word, vector)]\n",
    "train_neg_vector = list(map(lambda x: (x,glove_dict[x]), train_neg_entities)) # [(word, vector), ... (word, vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Statistics before delta optimzing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_cluster_pos</th>\n",
       "      <th>initial_cluster_neg</th>\n",
       "      <th>initial_cluster_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>52</td>\n",
       "      <td>0.785124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1742</td>\n",
       "      <td>436</td>\n",
       "      <td>0.799816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2066</td>\n",
       "      <td>505</td>\n",
       "      <td>0.803578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>154</td>\n",
       "      <td>0.869270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2204</td>\n",
       "      <td>490</td>\n",
       "      <td>0.818114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   initial_cluster_pos  initial_cluster_neg  initial_cluster_precision\n",
       "0                  190                   52                   0.785124\n",
       "1                 1742                  436                   0.799816\n",
       "2                 2066                  505                   0.803578\n",
       "3                 1024                  154                   0.869270\n",
       "4                 2204                  490                   0.818114"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_cluster = grant_to_cluster(train_pos_vector, 1, max_distance_each_cluster, centroids)\n",
    "false_cluster = grant_to_cluster(train_neg_vector, 1, max_distance_each_cluster, centroids)\n",
    "true_count, false_count = count_TPFP(centroids, true_cluster, false_cluster)\n",
    "\n",
    "result_list = []\n",
    "\n",
    "# 각 클러스터에 포함된 positive 엔티티, negative 엔티티수로 각 클러스터에 대한 precision 지표 산출\n",
    "for cluster in range(len(centroids)):\n",
    "    TP = true_count[cluster] # True Positive : 클러스터 포함되어있는 positive 엔티티\n",
    "    FP = false_count[cluster] # False Positive : 클러스터에 포함되어있는 negative 엔티티\n",
    "    if (TP+FP) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = TP / (TP+FP)\n",
    "    result_list.append([TP,FP,precision])\n",
    "\n",
    "# first_result_df\n",
    "pre_result_df = pd.DataFrame(result_list, columns=['initial_cluster_pos', \n",
    "                                                   'initial_cluster_neg', \n",
    "                                                   'initial_cluster_precision'])\n",
    "pre_result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply the adaptive clustering method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize delta for each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimaldeltas each clusters : \n",
      " [1.   0.6  0.6  0.74 0.6  0.6  1.   0.7  1.   0.62 1.   1.   1.   1.\n",
      " 1.   1.   1.   1.   0.61 1.  ]\n"
     ]
    }
   ],
   "source": [
    "P, R, F1, TP, FN, FP, initial_TP = get_ada_matrics(train_pos_vector, train_neg_vector, \n",
    "                                                   max_distance_each_cluster, centroids)\n",
    "\n",
    "\n",
    "TP_list = TP[np.array(F1).argmax(0),list(range(TP.shape[1]))]\n",
    "\n",
    "#cluster의 f1 score가 최대가 되는 지점의 delta를 optimal delta로 선정\n",
    "optimal_deltas = np.array(F1).argmax(0)\n",
    "optimal_deltas = (optimal_deltas * 0.01)+0.6\n",
    "\n",
    "# positive entity수가 0인 cluster는 delta를 0으로 변환\n",
    "optimal_deltas = np.array([d if tp != 0 else 0 for d, tp in zip(optimal_deltas, TP_list)])\n",
    "\n",
    "print('optimaldeltas each clusters : \\n', optimal_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_cluster_pos</th>\n",
       "      <th>initial_cluster_neg</th>\n",
       "      <th>initial_cluster_precision</th>\n",
       "      <th>opt_delta</th>\n",
       "      <th>opt_TP</th>\n",
       "      <th>opt_FP</th>\n",
       "      <th>opt_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>52</td>\n",
       "      <td>0.785124</td>\n",
       "      <td>1.00</td>\n",
       "      <td>190</td>\n",
       "      <td>52</td>\n",
       "      <td>0.785124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1742</td>\n",
       "      <td>436</td>\n",
       "      <td>0.799816</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1483</td>\n",
       "      <td>29</td>\n",
       "      <td>0.980820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2066</td>\n",
       "      <td>505</td>\n",
       "      <td>0.803578</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1828</td>\n",
       "      <td>26</td>\n",
       "      <td>0.985976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>154</td>\n",
       "      <td>0.869270</td>\n",
       "      <td>0.74</td>\n",
       "      <td>944</td>\n",
       "      <td>49</td>\n",
       "      <td>0.950655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2204</td>\n",
       "      <td>490</td>\n",
       "      <td>0.818114</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2033</td>\n",
       "      <td>77</td>\n",
       "      <td>0.963507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   initial_cluster_pos  initial_cluster_neg  initial_cluster_precision  \\\n",
       "0                  190                   52                   0.785124   \n",
       "1                 1742                  436                   0.799816   \n",
       "2                 2066                  505                   0.803578   \n",
       "3                 1024                  154                   0.869270   \n",
       "4                 2204                  490                   0.818114   \n",
       "\n",
       "   opt_delta  opt_TP  opt_FP  opt_precision  \n",
       "0       1.00     190      52       0.785124  \n",
       "1       0.60    1483      29       0.980820  \n",
       "2       0.60    1828      26       0.985976  \n",
       "3       0.74     944      49       0.950655  \n",
       "4       0.60    2033      77       0.963507  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구한 optimal delta를 cluster에 적용하여 각 cluster에 대한 precision, recall, TP, FP를 산출\n",
    "optimalP, optimalR, optimalTP, optimalFP = get_optimal_matrics(train_pos_vector, train_neg_vector, \n",
    "                                                               max_distance_each_cluster,centroids, \n",
    "                                                               optimal_deltas, initial_TP)\n",
    "\n",
    "matrics = []\n",
    "for m1, m2, m3, m4, m5 in zip(optimalP, optimalR,  optimalTP, optimalFP, optimal_deltas):\n",
    "    matrics.append([m1, m2, m3, m4, m5])\n",
    "    \n",
    "opt_delta_df = pd.DataFrame(matrics, columns=['opt_precision', 'opt_recall', 'opt_TP','opt_FP', 'opt_delta'])\n",
    "tmp_parse_df = opt_delta_df[['opt_delta','opt_TP','opt_FP', 'opt_precision']]\n",
    "train_result_df = pd.concat([pre_result_df, tmp_parse_df], axis = 1)\n",
    "train_result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Error triple detection using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test vectors from embedding model \n",
    "test_pos_vector = list(map(lambda x: (x,glove_dict[x]), test_pos_entities))\n",
    "test_neg_vector = list(map(lambda x: (x,glove_dict[x]), test_neg_entities))\n",
    "initial_TP = get_initial_TP(test_pos_vector, max_distance_each_cluster, centroids)\n",
    "\n",
    "# test 데이터의 embedding 벡터를 cluster에 할당하여 Precision, Recall 산출\n",
    "optimalP, optimalR, optimalTP, optimalFP = get_optimal_matrics(test_pos_vector, test_neg_vector, \n",
    "                                                               max_distance_each_cluster, centroids, \n",
    "                                                               optimal_deltas, initial_TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_cluster_pos</th>\n",
       "      <th>initial_cluster_neg</th>\n",
       "      <th>initial_cluster_precision</th>\n",
       "      <th>opt_delta</th>\n",
       "      <th>opt_TP</th>\n",
       "      <th>opt_FP</th>\n",
       "      <th>opt_precision</th>\n",
       "      <th>Test_TP</th>\n",
       "      <th>Test_FP</th>\n",
       "      <th>Test_precision</th>\n",
       "      <th>Test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>52</td>\n",
       "      <td>0.785124</td>\n",
       "      <td>1.00</td>\n",
       "      <td>190</td>\n",
       "      <td>52</td>\n",
       "      <td>0.785124</td>\n",
       "      <td>47</td>\n",
       "      <td>42</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1742</td>\n",
       "      <td>436</td>\n",
       "      <td>0.799816</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1483</td>\n",
       "      <td>29</td>\n",
       "      <td>0.980820</td>\n",
       "      <td>398</td>\n",
       "      <td>39</td>\n",
       "      <td>0.910755</td>\n",
       "      <td>0.836134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2066</td>\n",
       "      <td>505</td>\n",
       "      <td>0.803578</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1828</td>\n",
       "      <td>26</td>\n",
       "      <td>0.985976</td>\n",
       "      <td>422</td>\n",
       "      <td>24</td>\n",
       "      <td>0.946188</td>\n",
       "      <td>0.868313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>154</td>\n",
       "      <td>0.869270</td>\n",
       "      <td>0.74</td>\n",
       "      <td>944</td>\n",
       "      <td>49</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>217</td>\n",
       "      <td>45</td>\n",
       "      <td>0.828244</td>\n",
       "      <td>0.904167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2204</td>\n",
       "      <td>490</td>\n",
       "      <td>0.818114</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2033</td>\n",
       "      <td>77</td>\n",
       "      <td>0.963507</td>\n",
       "      <td>528</td>\n",
       "      <td>87</td>\n",
       "      <td>0.858537</td>\n",
       "      <td>0.910345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   initial_cluster_pos  initial_cluster_neg  initial_cluster_precision  \\\n",
       "0                  190                   52                   0.785124   \n",
       "1                 1742                  436                   0.799816   \n",
       "2                 2066                  505                   0.803578   \n",
       "3                 1024                  154                   0.869270   \n",
       "4                 2204                  490                   0.818114   \n",
       "\n",
       "   opt_delta  opt_TP  opt_FP  opt_precision  Test_TP  Test_FP  Test_precision  \\\n",
       "0       1.00     190      52       0.785124       47       42        0.528090   \n",
       "1       0.60    1483      29       0.980820      398       39        0.910755   \n",
       "2       0.60    1828      26       0.985976      422       24        0.946188   \n",
       "3       0.74     944      49       0.950655      217       45        0.828244   \n",
       "4       0.60    2033      77       0.963507      528       87        0.858537   \n",
       "\n",
       "   Test_recall  \n",
       "0     1.000000  \n",
       "1     0.836134  \n",
       "2     0.868313  \n",
       "3     0.904167  \n",
       "4     0.910345  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrics = []\n",
    "for m1, m2, m3, m4, m5 in zip(optimalP, optimalR, optimalTP, optimalFP, optimal_deltas):\n",
    "    matrics.append([m1, m2, m3, m4, m5])\n",
    "    \n",
    "output_df = pd.DataFrame(matrics, columns=['precision', 'recall', 'TP','FP', 'delta'])\n",
    "tmp_output_df = output_df[['TP','FP', 'precision', 'recall']]\n",
    "tmp_output_df.columns = ['Test_TP','Test_FP', 'Test_precision', 'Test_recall']\n",
    "\n",
    "train_test_df = pd.concat([train_result_df, tmp_output_df], axis = 1)\n",
    "train_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP : 4584\n",
      "FP : 1425\n",
      "precision : 0.763\n",
      "recall : 0.917\n",
      "F1 : 0.833\n",
      "\n",
      "Detected 3575 error triples\n",
      "Detected 416 normal triples as error triples\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대한 error detection결과 및 precision, recall, F1 score 산출\n",
    "test_TP = sum(train_test_df['Test_TP'].values)\n",
    "test_FP = sum(train_test_df['Test_FP'].values)\n",
    "\n",
    "test_precision = test_TP / (test_TP + test_FP)\n",
    "test_recall = test_TP / len(test_pos_entities)\n",
    "test_f1 = (2*test_precision*test_recall) / (test_precision+test_recall)\n",
    "\n",
    "print(f'TP : {test_TP}')\n",
    "print(f'FP : {test_FP}')\n",
    "print(f'precision : {test_precision:.3f}')\n",
    "print(f'recall : {test_recall:.3f}')\n",
    "print(f'F1 : {test_f1:.3f}')\n",
    "\n",
    "print(f'\\nDetected {len(test_neg_entities) - test_FP} error triples')\n",
    "print(f'Detected {len(test_pos_entities)-test_TP} normal triples as error triples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
